# ğŸ©º MEDICAL RAG CHATBOT  
## ğŸ“„ PDF-Based Medical Question Answering System using FAISS, LangChain & Ollama  

---

## ğŸš€ PROJECT OVERVIEW  

The **Medical RAG Chatbot** is a **Retrieval-Augmented Generation (RAG)** based application that answers medical questions **strictly from provided medical documents (PDFs)**.

This system does **NOT hallucinate answers** and does **NOT rely on the internet** for inference.  
All responses are generated using a **local Large Language Model (LLM)** powered by **Ollama** and a **FAISS vector database**.

âš ï¸ **Disclaimer**  
This project is intended **only for educational and academic purposes** and **must not be used as a replacement for professional medical advice**.

---

## ğŸ§  SYSTEM ARCHITECTURE & WORKFLOW  

Medical PDF Documents
â†“
Text Extraction
â†“
Text Chunking
â†“
Vector Embeddings
â†“
FAISS Vector Store
â†“
Retriever (LangChain)
â†“
Ollama Local LLM
â†“
Final Answer (CLI / Streamlit UI)


---

## ğŸ§± TECHNOLOGY STACK  

- ğŸ§  **LLM**: Ollama (Mistral / LLaMA3 / Phi-3)  
- ğŸ”— **Framework**: LangChain  
- ğŸ“¦ **Vector Database**: FAISS  
- ğŸ§¾ **Embeddings**: Sentence-Transformers  
- ğŸŒ **Frontend**: Streamlit  
- ğŸ **Programming Language**: Python  

---

## ğŸ“ PROJECT DIRECTORY STRUCTURE  



medical-chatbot/
â”‚
â”œâ”€â”€ app.py # Streamlit Web Application
â”œâ”€â”€ connect_memory.py # CLI-based Chatbot
â”œâ”€â”€ create_memory.py # PDF â†’ FAISS Vector Store Generator
â”œâ”€â”€ requirements.txt # Python Dependencies
â”œâ”€â”€ README.md # Project Documentation
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ *.pdf # Medical PDF Documents
â”‚
â””â”€â”€ vectorstore/
â””â”€â”€ db_faiss/
â”œâ”€â”€ index.faiss
â””â”€â”€ index.pkl


---

## ğŸ§© STEP 1: CREATE THE VECTOR STORE (MANDATORY STEP)  

This is the **first and most important step**.

### ğŸ” What happens in this step?  
- Medical PDF documents are loaded  
- Text is split into meaningful chunks  
- Each chunk is converted into vector embeddings  
- Embeddings are stored in a FAISS vector database  

### â–¶ï¸ Command to run  

```bash
python create_memory.py

ğŸ“‚ Input

Place all medical PDFs inside the data/ directory

ğŸ“¦ Output

FAISS vector store generated at:

vectorstore/db_faiss/

ğŸ§© STEP 2: CONNECT VECTOR STORE TO LLM (CLI CHATBOT)

This step connects:

FAISS vector store (memory)

Ollama local LLM

LangChain retrieval pipeline

â–¶ï¸ Run the CLI chatbot
python connect_memory.py

ğŸ§ª Example Interaction
Ask medical question: What is cancer?
Answer: Cancer is a disease in which abnormal cells divide uncontrollably...

ğŸ§© STEP 3: RUN THE STREAMLIT WEB APPLICATION ğŸŒ

This step provides a user-friendly web interface for the chatbot.

â–¶ï¸ Start Ollama (CPU mode recommended)
set OLLAMA_NO_CUDA=1
set OLLAMA_NUM_GPU=0
ollama serve

â–¶ï¸ Run the Streamlit app
streamlit run app.py

ğŸŒ Open in browser
http://localhost:8501

ğŸ–¥ï¸ STREAMLIT APPLICATION FEATURES

âœ… Real-time medical chatbot
âœ… Session-based chat history
âœ… Answers strictly from PDFs
âœ… Medical safety disclaimer
âœ… Fast FAISS retrieval
âœ… Clean and professional UI

ğŸ›¡ï¸ SAFETY & RELIABILITY

âŒ No internet-based inference

âŒ No hallucinated medical answers

âœ… Fully local execution

âœ… Privacy-preserving architecture

ğŸ“¦ INSTALLATION GUIDE
1ï¸âƒ£ Create a virtual environment (recommended)
conda create -n medi_chat python=3.10
conda activate medi_chat

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Install Ollama

ğŸ‘‰ https://ollama.com

4ï¸âƒ£ Pull a local model
ollama pull mistral

ğŸ§ª TESTED ENVIRONMENT

Windows 10 / 11

CPU-only systems

Conda environments

Python 3.9 â€“ 3.11
